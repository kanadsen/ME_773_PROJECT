{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING and assigning TRAIN,TEST and RUL DATA\n",
    "\"\"\" Import the turbofan training and test data and the test RUL values from the data files.\n",
    "    :param dataset_id: The dataset from turbofan to import\n",
    "    :return: A matrix with the training dataset, the test dataset and the test rul data\n",
    "\"\"\"\n",
    "def import_data(dataset_id):\n",
    "    train_initial_data = pd.read_csv('train_FD{}.txt'.format(dataset_id), sep=' ', header = None)  # Coverting txt file to csv\n",
    "    test_initial_data = pd.read_csv('test_FD{}.txt'.format(dataset_id), sep=' ', header = None)  # Coverting txt file to csv\n",
    "    RUL_initial_data = pd.read_csv('RUL_FD{}.txt'.format(dataset_id), sep=' ', header = None)  # Coverting txt file to csv\n",
    "    return train_initial_data,test_initial_data,RUL_initial_data\n",
    "train_initial_data,test_initial_data,RUL_values=import_data(str(input()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns containing NAN values\n",
    "train_initial_data=train_initial_data.drop([26,27],axis='columns')\n",
    "test_initial_data=test_initial_data.drop([26,27],axis='columns')\n",
    "\n",
    "train_initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_initial_data.columns = ['ENGINE_ID', 'Cycle_Time', 'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure5', 'SensorMeasure6','SensorMeasure7','SensorMeasure8','SensorMeasure9','SensorMeasure10','SensorMeasure11','SensorMeasure12','SensorMeasure13','SensorMeasure14','SensorMeasure15','SensorMeasure16','SensorMeasure17','SensorMeasure18','SensorMeasure19','SensorMeasure20','SensorMeasure21']\n",
    "test_initial_data.columns=['ENGINE_ID', 'Cycle_Time', 'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure5', 'SensorMeasure6','SensorMeasure7','SensorMeasure8','SensorMeasure9','SensorMeasure10','SensorMeasure11','SensorMeasure12','SensorMeasure13','SensorMeasure14','SensorMeasure15','SensorMeasure16','SensorMeasure17','SensorMeasure18','SensorMeasure19','SensorMeasure20','SensorMeasure21']\n",
    "train_initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_initial_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_initial_data=train_initial_data.drop(['ENGINE_ID','Cycle_Time'],axis=1)\n",
    "test_initial_data=test_initial_data.drop(['ENGINE_ID','Cycle_Time'],axis=1)\n",
    "train_initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Train dataset is scaled so that values are in the range 0 to 1\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_initial_data), \n",
    "                              columns=train_initial_data.columns, \n",
    "                              index=train_initial_data.index)\n",
    "train_data_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled = pd.DataFrame(scaler.transform(test_initial_data), \n",
    "                              columns=test_initial_data.columns, \n",
    "                              index=test_initial_data.index)\n",
    "test_data_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#n_components = 24 # How many dimensions you want to reduce to\n",
    "#pca = PCA(n_components=n_components, svd_solver= 'full')\n",
    "pca=PCA(0.95)\n",
    "train_data_PCA = pca.fit_transform(train_data_scaled)\n",
    "train_data_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_PCA = pd.DataFrame(train_data_PCA)\n",
    "train_data_PCA.index = train_data_scaled.index\n",
    "train_data_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_PCA = pca.transform(test_data_scaled)\n",
    "test_data_PCA = pd.DataFrame(test_data_PCA)\n",
    "test_data_PCA.index = test_data_scaled.index\n",
    "test_data_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all PCA components FOR THE scaled TRAINING SET\n",
    "train_FD001_PCA = pca.fit_transform(train_initial_data)\n",
    "train_FD001_PCA = pd.DataFrame(train_FD001_PCA)\n",
    "train_FD001_PCA.index = train_data_scaled.index\n",
    "\n",
    "# Project the scaled TEST SET onto the PCA space\n",
    "test_FD001_PCA = pca.transform(test_initial_data)\n",
    "test_FD001_PCA = pd.DataFrame(test_FD001_PCA)\n",
    "test_FD001_PCA.index = test_data_scaled.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061cc039d71cfce52f2b95acd05edf0f026f6f4266c793886cd1952f14f0a759"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
